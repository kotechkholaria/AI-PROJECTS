{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bit92618093cfab4818bf64aa8b16d42637",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Introduction\n",
    "\n",
    "\n",
    "Every year the lives of approximately 1.35 million people are cut short as a result of a road traffic crash. Between 20 and 50 million more people suffer non-fatal injuries, with many incurring a disability as a result of their injury.\n",
    "\n",
    "Road traffic injuries cause considerable economic losses to individuals, their families, and to nations as a whole. These losses arise from the cost of treatment as well as lost productivity for those killed or disabled by their injuries, and for family members who need to take time off work or school to care for the injured. Road traffic crashes cost most countries 3% of their gross domestic product.\n",
    "\n",
    "\n",
    "The objective of this project is to use the dataset on car accidents on Seattle,USAto explore the incident severityand to build a machine learning algorithm to predict the future accident severity to minimize the chance for car collisions which will predict the severity of an accident in a current weather and visibility conditions. when conditions are bad it should give us a notification to be more careful When conditions are bad, this model will alert drivers to remind them to be more careful.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "the data source is provided by Coursera for the same purpose. The dataset contains 37 different columns, a few redundant and a few not very impactful. So the first step after acquiring data was to process the data. Following columns were selected for the project: 'SEVERITYCODE', 'ADDRTYPE', 'INTKEY', 'COLLISIONTYPE', 'PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT', 'JUNCTIONTYPE', 'INATTENTIONIND', 'UNDERINFL', 'WEATHER', 'ROADCOND', 'LIGHTCOND', 'SPEEDING', 'HITPARKEDCAR' . All these selected columns then were converted to usable format by one-hot coding or converting categorical data into processable integer values to ease out the execution of problem. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-2806607f95e4>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-2806607f95e4>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    2.1\tData Cleaning\u001b[0m\n\u001b[1;37m       \t^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2.1\tData Cleaning\n",
    "\n",
    "There are a lot of problems with the data set keeping in mind that this is a machine learning project which uses classification to predict a categorical variable. The dataset has total observations of 194673 with variation in number of observations for every feature. First of all, the total dataset was high variation in the lengths of almost every column of the dataset. The dataset had a lot of empty\n",
    "columns which could have been beneficial had the data been present there. These columns included\n",
    "\n",
    "pedestrian granted way or not, segment lane key, cross walk key and hit parked car.\n",
    "\n",
    "\n",
    "The models aim was to predict the severity of an accident, considering that, the variable of Severity Code was in the form of 1 (Property Damage Only) and 2 (Injury Collision) which were encoded to the form of 0 (Property Damage Only) and 1 (Injury Collision). Furthermore, the Y was given value of 1 whereas N and no value was given 0 for the variables Inattention, Speeding and Under the influence. For lighting condition, Light was given 0 along with Medium as 1 and Dark as 2. For Road Condition, Dry was assigned 0, Mushy was assigned 1 and Wet was given 2. As for Weather Condition, 0 is Clear, Overcast is 1, Windy is 2 and Rain and Snow was given 3. 0 was assigned to the element of each variable which can be the least probable cause of severe accident whereas a high number represented adverse condition which can lead to a higher accident severity. Whereas, there were unique values for every variable which were either ‘Other’ or ‘Unknown’, deleting those rows entirely would have led to a lot of loss of data which is not preferred.\n",
    " \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "In order to deal with the issue of columns having a variation in frequency, arrays were made for each column which were encoded according to the original column and had equal proportion of elements as the original column. Then the arrays were imposed on the original columns in the positions which had ‘Other’ and ‘Unknown’ in them. This entire process of cleaning data led to a loss of almost 5000 rows which had redundant data, whereas other rows with unknown values were filled earlier.\n",
    "\n",
    "2.2\tFeature Selection\n",
    "\n",
    "A total of 5 features were selected for this project along with the target variable being Severity Code.\n",
    "\n",
    "Feature Variables\tDescription\n",
    "INATTENTIONIND\tWhether or not the driver was inattentive (Y/N)\n",
    "UNDERINFL\tWhether or not the driver was under the influence (Y/N)\n",
    "WEATHER\tWeather condition during time of collision (Overcast/Rain/Clear)\n",
    "ROADCOND\tRoad condition during the collision (Wet/Dry..)\n",
    "LIGHTCOND\tLight conditions during the collision (Lights On/Dark with light on)\n",
    "SPEEDING\tWhether the car was above the speed limit at the time of collision (Y/N)\n",
    " \n",
    "3.\tMethodology\n",
    "\n",
    "3.1\tData Collection\n",
    "\n",
    "The dataset used for this project is based on car accidents which have taken place within the city of Seattle, Washington from the year 2004 to 2020. This data is regarding car accidents the severity of each car accidents along with the time and conditions under which each accident occurred. The data set used for this project can be found here.\n",
    "\n",
    "3.2\tExploratory Analysis\n",
    "\n",
    "Considering that the feature set and the target variable are categorical variables with the likes of weather, road condition and light condition being an above level 2 categorical variables whose values are limited and usually based on a particular finite group whose correlation might depict a different image then what it actually is. Generally, considering the effect of these variables in car accidents are important hence these variables were selected. A few pictorial depictions of the dataset were made in order to better understand the data.\n",
    "\n",
    " \n",
    "The above figure illustrates, after data cleaning has taken place, the distribution of the target variables between Physical Injury and Property Damage Only. As it can be seen that the dataset is supervised but an unbalanced dataset where the distribution of the target variable is in almost 1:2 ratio in favor of property damage. It is very important to have a balanced dataset when using machine learning algorithms. Hence, SMOTE was used from imblearn library in order to balance the target variable in equal proportions in order to have an unbiased classification model which is trained on equal instances of both the elements under severity of accidents.\n",
    "\n",
    "\n",
    "As mentioned earlier, a number ‘0’ as an element of an independent variable is supposed to depict the least probable cause of a severe accident. The graph above is supposed to depict all the non- zero values within each independent variable of the model and can be seen as the frequency of adverse conditions under which accidents took place. The factor which had most number of accidents under adverse conditions was adverse weather conditions while adverse lighting condition had the second most number of accidents caused by it. The factors which contributed the least to an instance of an accident are over-speeding and the driver being under the influence.\n",
    " \n",
    "3.3\tMachine Learning Model Selection\n",
    "\n",
    "The machine learning models used are Logistic Regression, Decision Tree Analysis and k-Nearest Neighbor. Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable. The Decision Tree Analysis breaks down a data set into smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (based on distance). The reason why Decision Tree Analysis, Logistic Regression and k-Nearest Neighbor classification methods were chosen is because the Support Vector Machine (SVM) model is inaccurate for large data sets, while this data set has more than 180,000 rows filled with data. Furthermore, SVM works best with dataset filled with text and images.\n",
    "\n",
    "\n",
    "4.\tResults\n",
    "\n",
    "4.1\tDecision Tree Analysis\n",
    "\n",
    "Decision Tree Classifier from the scikit-learn library was used to run the Decision Tree Classification model on the Car Accident Severity data. The criterion chosen for the classifier was ‘entropy’ and the max depth was ‘6’. The post-SMOTE balanced data was used to predict and fit the Decision Tree Classifier.\n",
    " \n",
    "4.1.1\tClassification Report\n",
    "\n",
    "\tPrecision\tRecall\tf1-score\n",
    "0\t0.64\t0.72\t0.68\n",
    "1\t0.44\t0.34\t0.39\n",
    "Accuracy\t0.58\t\t\n",
    "Macro Avg\t0.54\t0.53\t0.53\n",
    "Weighted Avg\t0.56\t0.58\t0.56\n",
    "\n",
    "\n",
    "\n",
    "4.1.2\tConfusion Matrix\n",
    "\n",
    "\n",
    " \n",
    "4.2\tLogistic Regression\n",
    "\n",
    "Logistic Regression from the scikit-learn library was used to run the Logistic Regression Classification model on the Car Accident Severity data. The C used for regularization strength was ‘0.01’ whereas the solver used was ‘liblinear’. The post-SMOTE balanced data was used to predict and fit the Logistic Regression Classifier.\n",
    "\n",
    "4.2.1\tClassification Report\n",
    "\n",
    "\tPrecision\tRecall\tf1-score\n",
    "0\t0.72\t0.67\t0.69\n",
    "1\t0.35\t0.41\t0.38\n",
    "Accuracy\t0.59\t\t\n",
    "Macro Avg\t0.53\t0.54\t0.53\n",
    "Weighted Avg\t0.61\t0.59\t0.60\n",
    "Log Loss\t0.68\t\t\n",
    "4.2.2\tConfusion Matrix\n",
    "\n",
    "\n",
    " \n",
    "4.3\tk-Nearest Neighbor\n",
    "\n",
    "k-Nearest Neighbor classifier was used from the scikit-learn library to run the k-Nearest Neighbor machine learning classifier on the Car Accident Severity data. The best K, as shown below, for the model where the highest elbow bend exists is at 4. The post-SMOTE balanced data was used to predict and fit the k-Nearest Neighbor classifier.\n",
    "\n",
    "4.3.1\tBest kNN value\n",
    "\n",
    "\n",
    "4.3.2\tClassification Report\n",
    "\n",
    "\tPrecision\tRecall\tf1-score\n",
    "0\t0.93\t0.70\t0.80\n",
    "1\t0.08\t0.32\t0.13\n",
    "Accuracy\t0.67\t\t\n",
    "Macro Avg\t0.50\t0.51\t0.46\n",
    "Weighted Avg\t0.86\t0.67\t0.75\n",
    " \n",
    "5.\tDiscussion\n",
    "\n",
    "\n",
    "\n",
    "Decision Tree\t0.56\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k-Nearest Neighbor\t0.75\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5.1\tAverage f1-Score\n",
    "\n",
    "f1-score is a measure of accuracy of the model, which is the harmonic mean of the model’s precision and recall. Perfect precision and recall is shown by the f1-score as 1, which is the highest value for the f1-score, whereas the lowest possible value is 0 which means that either precision or recall is 0. The f1-score shown above is the average of the individual f1-scores of the two elements of the target variable i.e. Property Damage and Injury. When comparing the f1-scores of the three models, we can  see that k-Nearest Neighbor has the highest f1-score meaning that it has a higher precision and recall of the other two models. Whereas, the Decision Tree model’s f1-score is the lowest of the three at 0.56. Lastly, the f1-score of the Logistic Regression is at 0.60 which can be considered as an above average score. However, the average f1-score doesn’t depict the true picture of the models accuracy because of the different precision and recall of the model for both the elements of the target variable. Hence, it is biased more towards the precision and recall of Property Damage due to its weightage in the model.\n",
    " \n",
    "5.2\tPrecision\n",
    "\n",
    "Precision refers to the percentage of results which are relevant, in simpler terms it can be seen as how many of the selected items from the model are relevant. Mathematically, it is calculated by dividing true positives by true positive and false positive. The highest precision for Property Damage is for Logistic Regression, whereas for Injury it is the Decision Tree. The Precision is calculated individually above in order to understand how accurate the model is at predicting Property Damage and Injury individually. For the Decision Tree the precision of 0 is 0.64 and for 1 it is 0.44 which is fairly good. As for the Logistic Regression model, for 0 it is at 0.72 and for 1 it is 0.35. Lastly, for the k-Nearest Neighbor at 0 it is 0.93, which is highly accurate, however for 1 it is 0.08, extremely low. In terms of precision, the best performing model is the decision tree.\n",
    "\n",
    "5.3\tRecall\n",
    "\n",
    "Recall refers to the percentage of total relevant results correctly classified by the algorithm. In simpler terms, it tells how many relevant items were selected. It is calculated by dividing true positives by true positive and false negative. The highest precision for 0 is when using the k-Nearest Neighbor model at 0.70 as for 1 it is the Logistic Regression model at 0.41. The recall for both Property Damage and Injury is almost identical for the Decision Tree and k-Nearest Neighbor model. As for the Logistic Regression, the recall for Property Damage is 0.67 and for Injury it is 0.41. The recall for Property Damage and Injury is the most balanced in terms of being good for both the outputs of the target variable.\n",
    "\n",
    "\n",
    "6.\tConclusion\n",
    "\n",
    "When comparing all the models by their f1-scores, Precision and Recall, we can have a clearer picture in terms of the accuracy of the three models individually as a whole and how well they perform\n",
    " \n",
    "for each output of the target variable. When comparing these scores, we can see that the f1-score is highest for k-Nearest Neighbor at 0.75. However, later when we compare the precision and recall for each of the model, we can see that the k-Nearest Neighbor model performs poorly in the precision of 1 at 0.08. The variance is too high for the model to be selected as a viable option. When looking at the other two models, we can see that the Decision Tree has a more balanced precision for 0 and 1. Whereas, the Logistic Regression is more balanced when it comes to recall of 0 and 1. Furthermore, the average f1-score of the two models are very close but for the Logistic Regression it is higher by 0.04. It can be concluded that the both the models can be used side by side for the best performance.\n",
    "\n",
    "In retrospect, when comparing these scores to the benchmarks within the industry, it can be seen that they perform well but not as good as the benchmarks. These models could have performed better if a few more things were present and possible.\n",
    "\n",
    "•\tA balanced dataset for the target variable\n",
    "\n",
    "•\tMore instances recorded of all the accidents taken place in Seattle, Washington\n",
    "\n",
    "•\tLess missing values within the dataset for variables such as Speeding and Under the influence\n",
    "\n",
    "•\tMore factors, such as precautionary measures taken when driving, etc.\n",
    "\n",
    "\n",
    "7.\tRecommendations\n",
    "\n",
    "After assessing the data and the output of the Machine Learning models, a few recommendations can be made for the stakeholders. The developmental body for Seattle city can assess how much of these accidents have occurred in a place where road or light conditions were not ideal for that specific area and could launch development projects for those areas where most severe accidents take place in order to minimize the effects of these two factors. Whereas, the car drivers could also use\n",
    " \n",
    "this data to assess when to take extra precautions on the road under the given circumstances of light condition, road condition and weather, in order to avoid a severe accident, if any.\n",
    "\n",
    "7.1\tPublic Development Authority of Seattle (PDAS)\n",
    "\n",
    "\n",
    "\n",
    "Almost all of the accidents recorded have occurred on either a block or an intersection, the PDAS can take the following measures in response car accidents:\n",
    "•\tLaunch development projects for those areas where most severe accidents take place in order to minimize the effects of these two factors\n",
    "•\tIncreased investment towards improving lighting and road conditions of the area which have high instances recorded\n",
    "•\tInstall safety signs on the roads and ensure that all precautions are being taken by people within the area\n",
    " \n",
    "7.2\tCar Drivers\n",
    "\n",
    "A higher concentration of accidents can be mostly seen on the main roads of the city, specifically near the highway in the city center. The following steps can be taken by car drivers to avoid severe accidents:\n",
    "•\tBe extra careful around the I-5 highway which goes through the city center since it has the highest proportion of accidents recorded of total seattle\n",
    "•\tMost incidents occur under adverse weather, road and light conditions. Precautions should be taken under such circumstances, for e.g. driving slow on a wet road which may lead to loss of control\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "import scipy as sp\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,f1_score,log_loss,classification_report,confusion_matrix,jaccard_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import pydotplus\n",
    "import matplotlib.image as mpimg\n",
    "from io import StringIO\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import folium\n",
    "import webbrowser\n",
    "from folium import plugins\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Importing Dataset\n",
    "main_df=pd.read_csv('C:\\\\Users\\\\kchkh\\\\Desktop\\\\Capstone Project\\\\Car_Severity.csv')\n",
    "\n",
    "#Converting Severity Code from (1/2) tp (0/1)\n",
    "severity_code = main_df['SEVERITYCODE'].values\n",
    "\n",
    "labels = preprocessing.LabelEncoder()\n",
    "labels.fit([1, 2])\n",
    "severity_code = labels.transform (severity_code)\n",
    "\n",
    "main_df [\"SEVERITYCODE\"] = severity_code\n",
    "\n",
    "#Descriptive Stats\n",
    "descriptive_stats= main_df.describe(include=\"all\")\n",
    "\n",
    "    #Plotting counts of selected variables\n",
    "descriptive_stats_plot=descriptive_stats[[\"INATTENTIONIND\",\"UNDERINFL\",\"WEATHER\",\"ROADCOND\",\"LIGHTCOND\",\"SPEEDING\",\"SEVERITYCODE\"]]\n",
    "descriptive_stats_plot.drop(['unique','top','freq','mean','std','min','max','25%','50%','75%'],axis=0,inplace=True)\n",
    "descriptive_stats_plot=descriptive_stats_plot.transpose()\n",
    "\n",
    "color_yo=['sandybrown','sienna','sienna','sienna','sienna','sandybrown','sienna']\n",
    "descriptive_stats_plot.plot(kind='bar',alpha=0.70,color=[color_yo])\n",
    "plt.title('Number of entries in data for each variable - Seattle, Washington', fontsize=20, fontweight='bold')\n",
    "plt.xlabel(\"Variables\",fontsize=15,labelpad=20)\n",
    "plt.ylabel(\"Frequency\",fontsize=15,labelpad=20)\n",
    "plt.xticks(rotation=360)\n",
    "plt.show()\n",
    "\n",
    "#Area type of each accident\n",
    "explode_list = [0.05, 0.05, 0.2]\n",
    "color_list=['peachpuff','lightseagreen','darkorange']\n",
    "addtype=main_df['ADDRTYPE'].value_counts()\n",
    "\n",
    "addtype.plot(kind='pie',\n",
    "            figsize=(15, 6),\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            shadow=True,\n",
    "            labels=None,\n",
    "            pctdistance=1.12,\n",
    "            colors=color_list,\n",
    "            explode=explode_list)\n",
    "\n",
    "\n",
    "plt.title('Area of accident - Seattle, Washington', fontsize=18, y=1.05)\n",
    "plt.axis('equal')\n",
    "plt.legend(labels=addtype.index, loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Check IncKey unqiue numbers\n",
    "main_df['INCKEY'].nunique()\n",
    "\n",
    "#Encoding in attention (0 = No, 1 = Yes)\n",
    "main_df[\"INATTENTIONIND\"].replace(\"Y\", 1, inplace=True)\n",
    "main_df[\"INATTENTIONIND\"].replace(np.nan, 0, inplace=True)\n",
    "\n",
    "#Encoding Under the influence (0 = No, 1 = Yes)\n",
    "main_df[\"UNDERINFL\"].replace(\"N\", 0, inplace=True)\n",
    "main_df[\"UNDERINFL\"].replace(\"Y\", 1, inplace=True)\n",
    "\n",
    "#Encoding Speeding(0 = No, 1 = Yes)\n",
    "main_df[\"SPEEDING\"].replace(\"Y\", 1, inplace=True)\n",
    "main_df[\"SPEEDING\"].replace(np.nan, 0, inplace=True)\n",
    "\n",
    "#Encoding Light Conditions(0 = Light, 1 = Medium, 2 = Dark)\n",
    "main_df[\"LIGHTCOND\"].replace(\"Daylight\", 0, inplace=True)\n",
    "main_df[\"LIGHTCOND\"].replace(\"Dark - Street Lights On\", 1, inplace=True)\n",
    "main_df[\"LIGHTCOND\"].replace(\"Dark - No Street Lights\", 2, inplace=True)\n",
    "main_df[\"LIGHTCOND\"].replace(\"Dusk\", 1, inplace=True)\n",
    "main_df[\"LIGHTCOND\"].replace(\"Dawn\", 1, inplace=True)\n",
    "main_df[\"LIGHTCOND\"].replace(\"Dark - Street Lights Off\", 2, inplace=True)\n",
    "main_df[\"LIGHTCOND\"].replace(\"Dark - Unknown Lighting\", 2, inplace=True)\n",
    "main_df[\"LIGHTCOND\"].replace(\"Other\",\"Unknown\", inplace=True)\n",
    "\n",
    "#Encoding Weather Conditions(0 = Clear, 1 = Overcast and Cloudy, 2 = Windy, 3 = Rain and Snow\n",
    "main_df[\"WEATHER\"].replace(\"Clear\", 0, inplace=True)\n",
    "main_df[\"WEATHER\"].replace(\"Raining\", 3, inplace=True)\n",
    "main_df[\"WEATHER\"].replace(\"Overcast\", 1, inplace=True)\n",
    "main_df[\"WEATHER\"].replace(\"Other\", \"Unknown\", inplace=True)\n",
    "main_df[\"WEATHER\"].replace(\"Snowing\", 3, inplace=True)\n",
    "main_df[\"WEATHER\"].replace(\"Fog/Smog/Smoke\", 2, inplace=True)\n",
    "main_df[\"WEATHER\"].replace(\"Sleet/Hail/Freezing Rain\", 3, inplace=True)\n",
    "main_df[\"WEATHER\"].replace(\"Blowing Sand/Dirt\", 2, inplace=True)\n",
    "main_df[\"WEATHER\"].replace(\"Severe Crosswind\", 2, inplace=True)\n",
    "main_df[\"WEATHER\"].replace(\"Partly Cloudy\", 1, inplace=True)\n",
    "\n",
    "#Encoding Road Conditions(0 = Dry, 1 = Mushy, 2 = Wet)\n",
    "main_df[\"ROADCOND\"].replace(\"Dry\", 0, inplace=True)\n",
    "main_df[\"ROADCOND\"].replace(\"Wet\", 2, inplace=True)\n",
    "main_df[\"ROADCOND\"].replace(\"Ice\", 2, inplace=True)\n",
    "main_df[\"ROADCOND\"].replace(\"Snow/Slush\", 1, inplace=True)\n",
    "main_df[\"ROADCOND\"].replace(\"Other\", \"Unknown\", inplace=True)\n",
    "main_df[\"ROADCOND\"].replace(\"Standing Water\", 2, inplace=True)\n",
    "main_df[\"ROADCOND\"].replace(\"Sand/Mud/Dirt\", 1, inplace=True)\n",
    "main_df[\"ROADCOND\"].replace(\"Oil\", 2, inplace=True)\n",
    "\n",
    "\n",
    "#Making new dataframe with only variables and unique keys\n",
    "selected_columns=main_df[[\"X\",\"Y\",\"INCKEY\",\"INATTENTIONIND\",\"UNDERINFL\",\"WEATHER\",\"ROADCOND\",\"LIGHTCOND\",\"SPEEDING\",\"SEVERITYCODE\"]]\n",
    "feature_df=selected_columns.copy()\n",
    "feature_df.dropna(axis=0,how='any',inplace=True)\n",
    "feature_stats=feature_df.describe()\n",
    "\n",
    "np.count_nonzero(feature_df['UNDERINFL'])\n",
    "\n",
    "    #Light Condition\n",
    "lightcondsize = feature_df [\"LIGHTCOND\"].size\n",
    "\n",
    "featureinlightcond = feature_df ['LIGHTCOND'] == 'Unknown'\n",
    "\n",
    "lightcond = feature_df['LIGHTCOND']\n",
    "lightcond = lightcond.values\n",
    "lightcond = lightcond[featureinlightcond]\n",
    "\n",
    "lightcond[0:9036]=0\n",
    "lightcond[9036:13417]=1\n",
    "lightcond[13417:13961]=2\n",
    "\n",
    "feature_df.loc [feature_df.LIGHTCOND == \"Unknown\", 'LIGHTCOND'] = lightcond\n",
    "\n",
    "feature_df[\"LIGHTCOND\"]=feature_df[\"LIGHTCOND\"].astype(int)\n",
    "\n",
    "    #Road Condition\n",
    "roadcondsize = feature_df [\"ROADCOND\"].size\n",
    "\n",
    "featureinroadcond = feature_df ['ROADCOND'] == 'Unknown'\n",
    "\n",
    "roadcond = feature_df['LIGHTCOND']\n",
    "roadcond = roadcond.values\n",
    "roadcond = roadcond[featureinroadcond]\n",
    "\n",
    "roadcond[0:9954]=0\n",
    "roadcond[9954:10040]=1\n",
    "roadcond[10040:15163]=2\n",
    "\n",
    "feature_df.loc[feature_df.ROADCOND == \"Unknown\", 'ROADCOND'] = roadcond\n",
    "feature_df[\"ROADCOND\"]=feature_df[\"ROADCOND\"].astype(int)\n",
    "\n",
    "    #Weather Condition\n",
    "weathersize = feature_df [\"WEATHER\"].size\n",
    "\n",
    "featureinweather = feature_df ['WEATHER'] == 'Unknown'\n",
    "\n",
    "weather = feature_df['WEATHER']\n",
    "weather = weather.values\n",
    "weather = weather[featureinweather]\n",
    "\n",
    "weather[0:10151]=0\n",
    "weather[10151:12683]=1\n",
    "weather[12683:12742]=2\n",
    "weather[12742:15864]=3\n",
    "\n",
    "feature_df.loc[feature_df.WEATHER == \"Unknown\", 'WEATHER'] = weather\n",
    "feature_df[\"WEATHER\"]=feature_df[\"WEATHER\"].astype(int)\n",
    "\n",
    "    #Converting remaining to int\n",
    "feature_df[\"SPEEDING\"]=feature_df[\"SPEEDING\"].astype(int)\n",
    "feature_df[\"INATTENTIONIND\"]=feature_df[\"INATTENTIONIND\"].astype(int)\n",
    "feature_df[\"UNDERINFL\"]=feature_df[\"UNDERINFL\"].astype(int)\n",
    "\n",
    "\n",
    "#ML Feature Sets\n",
    "X=feature_df[[\"SPEEDING\",\"INATTENTIONIND\",\"UNDERINFL\",\"ROADCOND\",\"WEATHER\",\"LIGHTCOND\"]].values\n",
    "y=feature_df[[\"SEVERITYCODE\"]].values\n",
    "\n",
    "#Test/Train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)\n",
    "\n",
    "# Balance the Data\n",
    "os = SMOTE (random_state=0)\n",
    "os_data_X, os_data_y= os.fit_sample(X_train, y_train)\n",
    "\n",
    "#Make reduced df from feature_df to get a few random points to make map\n",
    "limit = 100005\n",
    "reduced_df = feature_df.iloc [0:limit:5, 0:]\n",
    "\n",
    "#Folium Map\n",
    "# let's start again with a clean copy of the map of San Francisco\n",
    "seattle_map = folium.Map(location=[47.61536892, -122.3302243], zoom_start=10)\n",
    "\n",
    "# instantiate a mark cluster object for the incidents in the dataframe\n",
    "incidents = plugins.MarkerCluster().add_to(seattle_map)\n",
    "\n",
    "# loop through the dataframe and add each data point to the mark cluster\n",
    "for lat, lng, label, in zip(reduced_df.Y, reduced_df.X, reduced_df.SEVERITYCODE):\n",
    "    folium.Marker(\n",
    "    location=[lat, lng],\n",
    "    icon=None,\n",
    "    popup=label,\n",
    "    ).add_to(incidents)\n",
    "\n",
    "seattle_map.add_child(incidents)\n",
    "\n",
    "# display map\n",
    "seattle_map\n",
    "seattle_map.save(\"seattlemap.html\")\n",
    "webbrowser.open(\"seattlemap.html\")\n",
    "\n",
    "#Decision Tree Clasifier\n",
    "DT = DecisionTreeClassifier(criterion=\"entropy\", max_depth=6)\n",
    "DT.fit(os_data_X,os_data_y)\n",
    "\n",
    "        #Make Prediction:\n",
    "yhatDT = DT.predict(X_test)\n",
    "\n",
    "        #Check Accuracy\n",
    "print('Accuracy score for Decision Tree = ', accuracy_score(yhatDT, y_test))\n",
    "\n",
    "        #Visualization\n",
    "print('Confusion Matrix - Decision Tree')\n",
    "print(pd.crosstab(y_test.ravel(), yhatDT.ravel(), rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "\n",
    "print(classification_report(yhatDT,y_test))\n",
    "\n",
    "        #COnfusion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    #Plot it\n",
    "cnf_matrix = confusion_matrix(y_test, yhatDT, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "        # Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Injury=1','Property Damage=0'],normalize= False,  title='Confusion matrix')\n",
    "\n",
    "#Logistic Regression\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear').fit(os_data_X,os_data_y)\n",
    "\n",
    "yhatLR = LR.predict(X_test)\n",
    "yhat_prob = LR.predict_proba(X_test)\n",
    "\n",
    "print(log_loss(y_test, yhat_prob))\n",
    "\n",
    "print (\"Accuracy\", accuracy_score(yhatLR,y_test))\n",
    "print (classification_report(y_test, yhatLR))\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, yhatLR, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "        # Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Injury=1','Property Damage=0'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}